\item \ref{cudac}: CUDA C/C++ is supported on NVIDIA GPUs through the \href{https://developer.nvidia.com/cuda-toolkit}{CUDA Toolkit}. First released in 2007, the toolkit covers nearly all aspects of the NVIDIA platform: an API for programming (incl. language extensions), libraries, tools for profiling and debugging, compiler, management tools, and more. The current version is CUDA 12.2. Usually, when referring to \emph{CUDA} without any additional context, the CUDA API is meant. While incorporating some Open Source components, the CUDA platform in its entirety is proprietary and closed sourced. The low-level CUDA instruction set architecture is PTX, to which higher languages like the CUDA C/C++ are translated to. PTX is compiled to SASS, the binary code executed on the device. As it is the reference for platform, the support for NVIDIA GPUs through CUDA C/C++ is very comprehensive. In addition to support through the CUDA toolkit, NVIDIA GPUs can also be \href{https://llvm.org/docs/CompileCudaWithLLVM.html}{used by Clang}, utilizing the LLVM toolchain to emit PTX code and compile it subsequently.
\item \ref{cudafortran}: CUDA Fortran, a proprietary Fortran extension by NVIDIA, is supported on NVIDIA GPUs via the \href{https://developer.nvidia.com/hpc-sdk}{NVIDIA HPC SDK} (\emph{NVHPC}). NVHPC implements most features of the CUDA API in Fortran and is activated through the \texttt{-cuda} switch in the \texttt{nvfortran} compiler. The CUDA extensions for Fortran are modeled closely after the CUDA C/C++ definitions. In addition to creating explicit kernels in Fortran, CUDA Fortran also supports \emph{cuf kernels}, a way to let the compiler generate GPU parallel code automatically. Very recently, \href{https://reviews.llvm.org/D150159}{CUDA Fortran support was also merged into Flang}, the LLVM-based Fortran compiler.
\item \ref{nvidiahip}: \href{https://github.com/ROCm-Developer-Tools/HIP}{HIP} programs can directly use NVIDIA GPUs via a CUDA backend. As HIP is strongly inspired by CUDA, the mapping is relatively straight-forward; API calls are named similarly (for example: \texttt{hipMalloc()} instead of \texttt{cudaMalloc()}) and keywords of the kernel syntax are identical. HIP also supports some CUDA libraries and creates interfaces to them (like \texttt{hipblasSaxpy()} instead of \texttt{cublasSaxpy()}). To target NVIDIA GPUs through the HIP compiler (\texttt{hipcc}), \texttt{HIP\_PLATFORM=nvidia} needs to be set in the environment. In order to initially create a HIP code from CUDA, AMD offers the \href{https://github.com/ROCm-Developer-Tools/HIPIFY}{HIPIFY} conversion tool.
\item \ref{nvidiahipfortran}: No Fortran version of HIP exists; HIP is solely a C/C++ model. But AMD offers an extensive set of ready-made interfaces to the HIP API and HIP and ROCm libraries with \href{https://github.com/ROCmSoftwarePlatform/hipfort}{hipfort} (MIT-licensed). All interfaces implement C functionality and CUDA-like Fortran extensions, for example to write kernels, are available.
\item \ref{nvidiasycl}: No direct support for \href{https://www.khronos.org/sycl/}{SYCL} is available by NVIDIA, but SYCL can be used on NVIDIA GPUs through multiple venues. First, SYCL can be \href{https://github.com/intel/llvm/blob/sycl/sycl/doc/GetStartedGuide.md\#build-dpc-toolchain-with-support-for-nvidia-cuda}{used through DPC++}, an Open-Source LLVM-based compiler project \href{https://github.com/intel/llvm}{led by Intel}. The DPC++ infrastructure is also available through Intel\textquotesingle s commercial \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html}{oneAPI toolkit} (\emph{Intel oneAPI DPC++/C++}) as \href{https://developer.codeplay.com/products/oneapi/nvidia/2023.2.1/guides/get-started-guide-nvidia}{a dedicated plugin}. Upstreaming SYCL support directly into LLVM is an \href{https://github.com/intel/llvm/issues/49}{ongoing effort}, which started \href{https://lists.llvm.org/pipermail/cfe-dev/2019-January/060811.html}{in 2019}. Further, SYCL can be used via \href{https://github.com/OpenSYCL/OpenSYCL/}{Open SYCL} (previously called hipSYCL), an independently developed SYCL implementation, using NVIDIA GPUs either through the CUDA support of LLVM or the \texttt{nvc++} compiler of NVHPC. A third popular possibility was the NVIDIA GPU support in \href{https://github.com/codeplaysoftware/sycl-for-cuda/tree/cuda}{ComputeCpp of CodePlay}; though \href{https://developer.codeplay.com/products/computecpp/ce/home/}{the product became unsupported in September 2023}. In case LLVM is involved, SYCL implementations can rely on CUDA support in LLVM, which needs the CUDA toolkit available for the final compilations parts beyond PTX. In order to translate a CUDA code to SYCL, Intel offers the \href{https://github.com/oneapi-src/SYCLomatic}{SYCLomatic} conversion tool.
\item \ref{syclfortran}: SYCL is a C++-based programming model (C++17) and by its nature does not support Fortran. Also, no pre-made bindings are available.
\item \ref{openaccc}: OpenACC C/C++ on NVIDIA GPUs is supported most extensively through the \href{https://developer.nvidia.com/hpc-sdk}{NVIDIA HPC SDK}. Beyond the bundled libraries, frameworks, and other models, the NVIDIA HPC SDK also features the \texttt{nvc}/\texttt{nvc++} compilers, in which \href{https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html\#acc-use}{OpenACC support} can be enabled with the \texttt{-acc\ -gpu}. The support of OpenACC in this vendor-delivered compiler is very comprehensive, it conforms to version 2.7 of the specification. A variety of compile options are available to modify the compilation process. In addition to NVIDIA HPC SDK, good support is also available in GCC since GCC 5.0, \href{https://gcc.gnu.org/wiki/OpenACC}{supporting OpenACC 2.6} through the \texttt{nvptx} architecture. The compiler switch to enable OpenACC in \texttt{gcc}/\texttt{g++} is \texttt{-fopenacc}, further options are available. Further, the \href{https://csmd.ornl.gov/project/clacc}{Clacc compiler} implements OpenACC support into the LLVM toolchain, adapting the Clang frontend. As a central design aspect, it translates OpenACC to OpenMP as part of the compilation process. OpenACC can be activated in a Clacc-\texttt{clang} via \texttt{-fopenacc}, and further compiler options exist, mostly leveraging OpenMP options. A recent study by \href{https://ieeexplore.ieee.org/document/10029456}{Jarmusch et al.} compared these compilers for coverage of the OpenACC 3.0 specification.
\item \ref{openaccfortran}: Support of OpenACC Fortran on NVIDIA GPUs is similar to OpenACC C/C++, albeit not identical. First, \href{https://developer.nvidia.com/hpc-sdk}{NVIDIA HPC SDK} supports OpenACC in Fortran through the included \texttt{nvfortran} compiler, with options like for the C/C++ compilers. In addition, also \href{https://gcc.gnu.org/wiki/OpenACC}{GCC supports OpenACC} through the \texttt{gfortran} compiler with identical compiler options to the C/C++ compilers. Further, similar to OpenACC support in LLVM for C/C++ through \emph{Clacc} contributions, the LLVM frontend for Fortran, \href{https://flang.llvm.org/docs/}{Flang} (the successor of \emph{F18}, not \emph{classic Flang}), \href{https://flang.llvm.org/docs/OpenACC.html}{supports OpenACC} as well. Support was initially contributed through the \href{https://ieeexplore.ieee.org/document/9651310}{Flacc project} and now resides in the main LLVM project. Finally, the \href{https://www.hpe.com/psnow/doc/a50002303enw}{HPE Cray Programming Environment} supports \href{https://cpe.ext.hpe.com/docs/cce/man7/intro_openacc.7.html}{OpenACC Fortran}; in \texttt{ftn}, OpenACC can be enabled through \texttt{-hacc}.
\item \ref{nvidiaopenmpc}: OpenMP in C/C++ is supported on NVIDIA GPUs (\emph{Offloading}) through multiple venues, similarly to OpenACC. First, the NVIDIA HPC SDK supports \href{https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html\#openmp-use}{OpenMP GPU offloading} in both \texttt{nvc} and \texttt{nvc++}, albeit only a subset of the entire OpenMP 5.0 standard (see \href{https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html\#openmp-subset}{the documentation for supported/unsupported features}). The key compiler option is \texttt{-mp}. Also in GCC, \href{https://gcc.gnu.org/wiki/Offloading}{OpenMP offloading} can be used to NVIDIA GPUs; the compiler switch is \texttt{-fopenmp}, with options delivered through \href{https://gcc.gnu.org/onlinedocs/gcc/C-Dialect-Options.html\#index-foffload}{\texttt{-foffload} and \texttt{-foffload-options}}. GCC \href{https://gcc.gnu.org/onlinedocs/gcc-13.1.0/libgomp/OpenMP-Implementation-Status.html}{currently supports OpenMP 4.5 entirely}, while OpenMP features of 5.0, 5.1, and, 5.2 are currently being implemented. Similarly in Clang, where \href{https://clang.llvm.org/docs/OffloadingDesign.html}{OpenMP offloading to NVIDIA GPUs} is supported and enabled through \texttt{-fopenmp\ -fopenmp-targets=nvptx64}, with offload architectures selected via \texttt{-\/-offload-arch=native} (or similar). Clang implements \href{https://clang.llvm.org/docs/OpenMPSupport.html\#openmp-implementation-details}{nearly all OpenMP 5.0 features and most of OpenMP 5.1/5.2}. In the HPE Cray Programming Environment, a \href{https://cpe.ext.hpe.com/docs/cce/man7/intro_openmp.7.html}{subset of OpenMP 5.0/5.1 is supported} for NVIDIA GPUs. It can be activated through \texttt{-fopenmp}. Also \href{https://github.com/ROCm-Developer-Tools/aomp/}{AOMP}, AMD\textquotesingle s Clang/LLVM-based compiler, supports NVIDIA GPUs. Support of OpenMP features in the compilers was recently discussed in the \href{https://www.openmp.org/wp-content/uploads/2022_ECP_Community_BoF_Days-OpenMP_RoadMap_BoF.pdf}{OpenMP ECP BoF 2022}.
\item \ref{nvidiaopenmpfortran}: OpenMP in Fortran is supported on NVIDIA GPUs nearly identical to C/C++. \href{https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html\#openmp-use}{NVIDIA HPC SDK\textquotesingle s \texttt{nvfortran}} implements support, \href{https://gcc.gnu.org/wiki/openmp}{GCC\textquotesingle s \texttt{gfortran}}, \href{https://flang.llvm.org/docs/}{LLVM\textquotesingle s Flang} (through \texttt{-mp}, and \href{https://flang.llvm.org/docs/GettingStarted.html\#openmp-target-offload-build}{only when Flang is compiled via Clang}), and also the \href{https://cpe.ext.hpe.com/docs/cce/man7/intro_openmp.7.html}{HPE Cray Programming Environment}.
\item \ref{nvidiastandardc}: Standard language parallelism of C++, namely algorithms and data structures of the \emph{parallel STL}, is supported on NVIDIA GPUs \href{https://docs.nvidia.com/hpc-sdk/compilers/c++-parallel-algorithms/index.html}{through the \texttt{nvc++} compiler of the NVIDIA HPC SDK}. The key compiler option is \texttt{-stdpar=gpu}, which enables offloading of parallel algorithms to the GPU. Also, AdaptiveCpp \href{https://github.com/AdaptiveCpp/AdaptiveCpp/blob/develop/doc/stdpar.md}{supports pSTL algorithms}, enabled via \texttt{-\/-acpp-stdpar}. Further, \href{https://intel.github.io/llvm-docs/GetStartedGuide.html\#build-dpc-toolchain-with-support-for-nvidia-cuda}{NVIDIA GPUs can be targeted from Intel\textquotesingle s DPC++ compiler}, enabling usage of pSTL algorithms implemented in Intel\textquotesingle s Open Source \href{https://github.com/oneapi-src/oneDPL}{oneDPL} (\emph{oneAPI DPC++ Library}) on NVIDIA GPUs. Finally, a \href{https://discourse.llvm.org/t/rfc-openmp-offloading-backend-for-c-parallel-algorithms/73468}{current proposal in the LLVM community} aims at implementing pSTL support through an OpenMP backend.
\item \ref{nvidiastandardfortran}: Standard language parallelism of Fortran, mainly \texttt{do\ concurrent}, is supported on NVIDIA GPUs \href{https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/}{through the \texttt{nvfortran} compiler of the NVIDIA HPC SDK}. As for the C++ case, it is enabled through the \texttt{-stdpar=gpu} compiler option.
\item \ref{nvidiakokkosc}: \href{https://github.com/kokkos/kokkos}{Kokkos} supports NVIDIA GPUs in C++. Kokkos has \href{https://kokkos.github.io/kokkos-core-wiki/requirements.html}{multiple backends} available with NVIDIA GPU support: a native CUDA C/C++ backend (using \texttt{nvcc}), an NVIDIA HPC SDK backend (using CUDA support in \texttt{nvc++}), and a Clang backend, using either Clang\textquotesingle s CUDA support directly or \href{https://docs.nersc.gov/development/programming-models/kokkos/}{via the OpenMP offloading facilities} (via \texttt{clang++}).
\item \ref{nvidiakokkosfortran}: Kokkos is a C++ programming model, but an official compatibility layer for Fortran (\href{https://github.com/kokkos/kokkos-fortran-interop}{\emph{Fortran Language Compatibility Layer}, FLCL}) is available. Through this layer, GPUs can be used as supported by Kokkos C++.
\item \ref{nvidiaalpakac}: \href{https://github.com/alpaka-group/alpaka}{Alpaka} supports NVIDIA GPUs in C++ (C++17), either through the NVIDIA CUDA C/C++ compiler \texttt{nvcc} or LLVM/Clang\textquotesingle s support of CUDA in \texttt{clang++}.
\item \ref{nvidiaalpakafortran}: Alpaka is a C++ programming model and no ready-made Fortran support exists.
\item \ref{nvidiapython}: Using NVIDIA GPUs from Python code can be achieved through multiple venues. NVIDIA itself offers \href{https://github.com/NVIDIA/cuda-python}{CUDA Python}, a package delivering low-level interfaces to CUDA C/C++. Typically, code is not directly written using CUDA Python, but rather CUDA Python functions as a backend for higher level models. CUDA Python is available on PyPI as \href{https://pypi.org/project/cuda-python/}{\texttt{cuda-python}}. An alternative to CUDA Python from the community is \href{https://github.com/inducer/pycuda}{PyCUDA}, which adds some higher-level features and functionality and comes with its own C++ base layer. PyCUDA is available on PyPI as \href{https://pypi.org/project/pycuda/}{\texttt{pycuda}}. The most well-known, higher-level abstraction is \href{https://cupy.dev/}{CuPy}, which implements primitives known from Numpy with GPU support, offers functionality for defining custom kernels, and bindings to libraries. CuPy is available on PyPI as \href{https://pypi.org/project/cupy-cuda12x/}{\texttt{cupy-cuda12x}} (for CUDA 12.x). Two packages arguably providing even higher abstractions are Numba and CuNumeric. \href{http://numba.pydata.org/}{Numba} offers access to NVIDIA GPUs and features acceleration of functions through Python decorators (\emph{functions wrapping functions}); it is available as \href{https://pypi.org/project/numba/}{\texttt{numba}} on PyPI. \href{https://github.com/nv-legate/cunumeric}{cuNumeric}, a project by NVIDIA, allows to access the GPU via Numpy-inspired functions (like CuPy), but utilizes the \href{https://github.com/nv-legate/legate.core}{Legate library} to transparently scale to multiple GPUs.
\item \ref{amdcudac}: While CUDA is generally not directly supported on AMD GPUs, it can be translated to HIP through AMD\textquotesingle s \href{https://github.com/ROCm-Developer-Tools/HIPIFY}{HIPIFY}. Using \texttt{hipcc} and \texttt{HIP\_PLATFORM=amd} in the environment, CUDA-to-HIP-translated code can be executed. In addition, a third-party, open source library/framework is currently being developed to execute CUDA code on AMD GPUs, without source code modifications, called \href{https://github.com/vosen/ZLUDA}{ZLUDA}.
\item \ref{amdcudafortran}: No direct support for CUDA Fortran on AMD GPUs is available, but AMD offers a source-to-source translator, \href{https://github.com/ROCmSoftwarePlatform/gpufort}{GPUFORT}, to convert some CUDA Fortran to either Fortran with OpenMP (via \href{https://github.com/ROCm-Developer-Tools/aomp}{AOMP}) or Fortran with HIP bindings and extracted C kernels (via \href{https://github.com/ROCmSoftwarePlatform/hipfort}{hipfort}). As stated in the project repository, the covered functionality is \href{https://github.com/ROCmSoftwarePlatform/gpufort\#limitations}{driven by use-case requirements}; the last commit is two years old.
\item \ref{amdhipc}: \href{https://github.com/ROCm-Developer-Tools/HIP}{HIP} C++ is the \emph{native} programming model for AMD GPUs and, as such, fully supports the devices. It is part of AMD\textquotesingle s GPU-targeted \href{https://rocm.docs.amd.com/en/latest/}{ROCm platform}, which includes compilers, libraries, tool, and drivers and mostly consists of Open Source Software. HIP code can be compiled with \href{https://github.com/ROCm-Developer-Tools/HIPCC}{\texttt{hipcc}}, utilizing the correct environment variables (like \texttt{HIP\_PLATFORM=amd}) and compiler options (like \texttt{-\/-offload-arch=gfx90a}). \texttt{hipcc} is a \emph{compiler driver} (wrapper script) which assembles the correct compilation string, finally calling \href{https://github.com/RadeonOpenCompute/llvm-project}{AMD\textquotesingle s Clang compiler} to generate host/device code (using the \href{https://llvm.org/docs/AMDGPUUsage.html}{AMDGPU backend}).
\item \ref{amdsyclc}: No direct support for SYCL is available by AMD for their GPU devices. But like for the NVIDIA ecosystem, SYCL C++ can be used on AMD GPUs through third-party software. First, \href{https://github.com/OpenSYCL/OpenSYCL}{Open SYCL} (previously \emph{hipSYCL}) supports AMD GPUs, relying on HIP/ROCm support in Clang. All available \href{https://github.com/OpenSYCL/OpenSYCL/blob/develop/doc/compilation.md}{internal compilation models} can target AMD GPUs. Second, also AMD GPUs can be targeted through both \href{https://github.com/intel/llvm/blob/sycl/sycl/doc/GetStartedGuide.md\#build-dpc-toolchain-with-support-for-hip-amd}{DPC++}, Intel\textquotesingle s LLVM-based Open Source compiler, and the commercial version included in the \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html}{oneAPI toolkit} (via an \href{https://developer.codeplay.com/products/oneapi/amd/2023.2.1/guides/get-started-guide-amd}{AMD ROCm plugin}). In comparison to SYCL support for CUDA, no conversion tool like SYCLomatic exists.
\item \ref{amdopenaccc}: OpenACC C/C++ is not supported by AMD itself, but third-party support is available for AMD GPUs through GCC or Clacc (similarly to their support of OpenACC C/C++ for NVIDI GPUS). In \href{https://gcc.gnu.org/wiki/Offloading}{GCC, OpenACC support} can be activated through \texttt{-fopenacc}, and further specified for AMD GPUs with, for example, \texttt{-foffload=amdgcn-amdhsa="-march=gfx906"}. \href{https://csmd.ornl.gov/project/clacc}{Clacc also supports OpenACC C/C++ on AMD GPUs} by translating OpenACC to OpenMP and using LLVM\textquotesingle s AMD support. The enabling compiler switch is \texttt{-fopenacc}, and AMD GPU targets can be further specified by, for example, \texttt{-fopenmp-targets=amdgcn-amd-amdhsa}. \href{/\%22https://github.com/intel/intel-application-migration-tool-for-openacc-to-openmp/\%22}{Intel\textquotesingle s OpenACC to OpenMP source-to-source translator} can also be used for AMD\textquotesingle s platform.
\item \ref{amdopenaccfortran}: No native support for OpenACC on AMD GPUs for Fortran is available, but AMD supplies \href{https://github.com/ROCmSoftwarePlatform/gpufort}{GPUFORT}, a research project to source-to-source translate OpenACC Fortran to either Fortran with added OpenMP or Fortran with HIP bindings and extracted C kernels (using \href{https://github.com/ROCmSoftwarePlatform/hipfort}{hipfort}). The covered functionality of GPUFORT is driven by use-case requirements, the last commit is two years old. Support for OpenACC Fortran is also available by the community through \href{https://gcc.gnu.org/onlinedocs/gfortran/OpenACC.html}{GCC (\texttt{gfortran})} and upcoming in \href{https://ieeexplore.ieee.org/document/9651310}{LLVM (Flacc)}. Also the \href{https://cpe.ext.hpe.com/docs/cce/man7/intro_openacc.7.html}{HPE Cray Programming Environment supports OpenACC Fortran} on AMD GPUs. In addition, the \href{https://github.com/intel/intel-application-migration-tool-for-openacc-to-openmp}{translator tool to convert OpenACC source to OpenMP source by Intel} can be used.
\item \ref{amdopenmpc}: AMD offers \href{https://github.com/ROCm-Developer-Tools/aomp}{AOMP}, a dedicated, Clang-based compiler for using OpenMP C/C++ on AMD GPUs (\emph{offloading}). AOMP is usually shipped with ROCm. The compiler \href{https://www.exascaleproject.org/wp-content/uploads/2022/02/Elwasif-ECP-sollve_vv_final.pdf}{supports most OpenMP 4.5 and some OpenMP 5.0 features}. Since the compiler is Clang-based, the usual Clang compiler options apply (\texttt{-fopenmp} to enable OpenMP parsing, and others). Also in the upstream Clang compiler, \href{https://clang.llvm.org/docs/OffloadingDesign.html}{AMD GPUs can be targeted through OpenMP}; as outlined for NVIDIA GPUs, the support for OpenMP 5.0 is nearly complete, and support for OpenMP 5.1/5.2 is comprehensive. In addition, the \href{https://cpe.ext.hpe.com/docs/cce/man7/intro_openmp.7.html}{HPE Cray Programming Environment} supports OpenMP on AMD GPUs.
\item \ref{amdopenmpfortran}: Through \href{https://github.com/ROCm-Developer-Tools/aomp}{AOMP}, AMD supports OpenMP offloading to AMD GPUs in Fortran, using the \texttt{flang} executable and Clang-typical compiler options (foremost \texttt{-fopenmp}). Support for AMD GPUs is also available through the \href{https://cpe.ext.hpe.com/docs/cce/man7/intro_openmp.7.html}{HPE Cray Programming Environment}.
\item \ref{amdstandardc}: AMD does not yet provide production-grade support for Standard-language parallelism in C++ for their GPUs. Currently under development is \href{https://github.com/ROCmSoftwarePlatform/roc-stdpar}{\emph{roc-stdpar}} (ROCm Standard Parallelism Runtime Implementation), which aims to supply pSTL algorithms on the GPU and \href{https://discourse.llvm.org/t/rfc-adding-c-parallel-algorithm-offload-support-to-clang-llvm/72159}{merge the implementation with upstream LLVM}. Support for GPU-parallel algorithms is enabled with \texttt{-stdpar}. An \href{https://discourse.llvm.org/t/rfc-openmp-offloading-backend-for-c-parallel-algorithms/73468}{alternative proposal in the LLVM} community aims to support the pSTL via an OpenMP backend. Also AdaptiveCpp \href{https://github.com/AdaptiveCpp/AdaptiveCpp/blob/develop/doc/stdpar.md}{supports C++ parallel algorithms} via a \texttt{-\/-acpp-stdpar} switch. Intel provides the Open Source \href{https://github.com/oneapi-src/oneDPL}{oneDPL} (\emph{oneAPI DPC++ Library}) which \href{https://oneapi-src.github.io/oneDPL/parallel_api_main.html}{implements pSTL algorithms} through the DPC++ compiler (see also \emph{C++ Standard Parallelism for Intel GPUs}). DPC++ has \href{https://intel.github.io/llvm-docs/GetStartedGuide.html\#build-dpc-toolchain-with-support-for-hip-amd}{experimental support for AMD GPUs}.
\item \ref{amdstandardfortran}: Recently, standard parallelism in Fortran is supported on AMD GPUs via AMD\textquotesingle s \emph{Next Gen} Fortran Compiler (based on the new LLVM Flang). Support is being improved, a summary is \href{https://github.com/amd/InfinityHub-CI/blob/main/fortran/README.md}{available on GitHub}. Also in \href{https://support.hpe.com/hpesc/public/docDisplay?docId=dp00005037en_us&docLocale=en_US}{HPE\textquotesingle s Cray Fortran compiler}, Standard parallelism is supported.
\item \ref{amdkokkosc}: \href{https://github.com/kokkos/kokkos}{Kokkos} supports AMD GPUs in C++ mainly through the HIP/ROCm backend. Also, an OpenMP offloading backend is available.
\item \ref{amdalpakac}: \href{https://github.com/alpaka-group/alpaka}{Alpaka} supports AMD GPUs in C++ through HIP or through an OpenMP backend.
\item \ref{amdpython}: AMD supports GPU programming with Python via \href{https://github.com/ROCm/hip-python}{HIP Python}, providing low-level bindings for HIP. \href{https://docs.cupy.dev/en/latest/install.html\#using-cupy-on-amd-gpu-experimental}{CuPy} experimentally supports AMD GPUs/ROCm. The package can be found on PyPI as \texttt{cupy-rocm-5-0}. Numba once had \href{https://numba.pydata.org/numba-doc/latest/roc/index.html}{support for AMD GPUs}, but it is \href{https://numba.readthedocs.io/en/stable/release-notes.html\#version-0-54-0-19-august-2021}{not maintained anymore}. Low-level bindings from Python to HIP exist, for example \href{https://github.com/jatinx/PyHIP}{PyHIP} (available as \texttt{pyhip-interface} on PyPI). Bindings to OpenCL also exist (\href{https://documen.tician.de/pyopencl/}{PyOpenCL}).
\item \ref{intelcudac}: Intel itself does not support CUDA C/C++ on their GPUs. They offer \href{https://github.com/oneapi-src/SYCLomatic}{SYCLomatic}, though, an Open Source tool to translate CUDA code to SYCL code, allowing it to run on Intel GPUs. The commercial variant of SYCLomatic is called the \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compatibility-tool.html}{DPC++ Compatibility Tool} and bundled with oneAPI toolkit. The community project \href{https://github.com/CHIP-SPV/chipStar}{chipStar} (previously called CHIP-SPV, recently released a 1.0 version) allows to target Intel GPUs from CUDA C/C++ code by using the CUDA support in Clang. chipStar delivers a \href{https://github.com/CHIP-SPV/chipStar/blob/main/docs/Using.md\#compiling-cuda-application-directly-with-chipstar}{Clang-wrapper, \texttt{cucc}}, which replaces calls to \texttt{nvcc}.
\item \ref{intelcudafortran}: No direct support exists for CUDA Fortran on Intel GPUs. A simple example to bind SYCL to a (CUDA) Fortran program (via ISO C BINDING) can be \href{https://github.com/codeplaysoftware/SYCL-For-CUDA-Examples/tree/master/examples/fortran_interface}{found on GitHub}.
\item \ref{intelhipc}: No native support for HIP C++ on Intel GPUs exists. The Open Source third-party project \href{https://github.com/CHIP-SPV/chipStar}{chipStar} (previously called CHIP-SPV), though, supports \href{https://github.com/CHIP-SPV/chipStar/blob/main/docs/Using.md\#compiling-a-hip-application-using-chipstar}{HIP on Intel GPUs} by mapping it to OpenCL or Intel\textquotesingle s Level Zero runtime. The compiler uses an LLVM-based toolchain and relies on its HIP and SPIR-V functionality.
\item \ref{intelhipfortran}: HIP for Fortran does not exist, and also no translation efforts for Intel GPUs.
\item \ref{intelsyclc}: \href{https://www.khronos.org/sycl/}{SYCL} is a C++17-based standard and selected by Intel as the prime programming model for Intel GPUs. Intel implements SYCL support for their GPUs \href{https://github.com/intel/llvm}{via DPC++}, an LLVM-based compiler toolchain. Currently, Intel maintains an own fork of LLVM, but \href{https://lists.llvm.org/pipermail/cfe-dev/2019-January/060811.html}{plans to upstream the changes} to the main LLVM repository. Based on DPC++, Intel releases a \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html}{commercial \emph{Intel oneAPI DPC++} compiler} as part of the \href{https://www.intel.com/content/www/us/en/developer/tools/oneapi/toolkits.html}{oneAPI toolkit}. The third-party project Open SYCL also supports Intel GPUs, by leveraging/creating LLVM support (either SPIR-V or Level Zero). A previous solution for targeting Intel GPUs from SYCL was \href{https://developer.codeplay.com/products/computecpp/ce/home/}{ComputeCpp of CodePlay}. The project became unsupported in September 2023 (in favor of implementations to the DPC++ project).
\item \ref{intelopenaccc}: No direct support for OpenACC C/C++ is available for Intel GPUs. Intel offers a Python-based tool to translate source files with OpenACC C/C++ to OpenMP C/C++, the \href{https://github.com/intel/intel-application-migration-tool-for-openacc-to-openmp}{\emph{Application Migration Tool for OpenACC to OpenMP API}}.
\item \ref{intelopenaccfortran}: Also for OpenACC Fortran, no direct support is available for Intel GPUs. Intel\textquotesingle s \href{https://github.com/intel/intel-application-migration-tool-for-openacc-to-openmp}{source-to-source translation tool from OpenACC to OpenMP} also supports Fortran, though.
\item \ref{intelopenmpc}: OpenMP is a second key programming model for Intel GPUs and \href{https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-cpp-fortran-compiler-openmp/top.html}{well-supported by Intel}. For C++, the support is built into the commercial version of DPC++/C++, \emph{Intel oneAPI DPC++/C++}. All \href{https://www.intel.com/content/www/us/en/developer/articles/technical/openmp-features-and-extensions-supported-in-icx.html}{OpenMP 4.5 and most OpenMP 5.0 and 5.1 features are supported}. OpenMP can be enabled through the \texttt{-qopenmp} compiler option of \texttt{icpx}; a suitable offloading target can be given via \texttt{-fopenmp-targets=spir64}.
\item \ref{intelopenmpfortran}: OpenMP in Fortran is Intel\textquotesingle s main selected route to bring Fortran applications to their GPUs. OpenMP offloading in Fortran is supported through \href{https://www.intel.com/content/www/us/en/docs/fortran-compiler/developer-guide-reference/2023-2/overview.html}{Intel\textquotesingle s Fortran Compiler \texttt{ifx}} (the new LLVM-based version, not the \emph{Fortran Compiler Classic}), part of the oneAPI HPC Toolkit. Similarly to C++, OpenMP offloading can be enabled through a combination of \texttt{-qopenmp} and \texttt{-fopenmp-targets=spir64}.
\item \ref{intelstandardc}: Intel supports C++ standard parallelism (\emph{pSTL}) through the Open Source \href{https://oneapi-src.github.io/oneDPL/index.html}{oneDPL} (oneAPI DPC++ Library), also available as part of the oneAPI toolkit. It \href{https://oneapi-src.github.io/oneDPL/parallel_api_main.html}{implements the pSTL} on top of the DPC++ compiler, algorithms, data structures, and policies live in the \texttt{oneapi::dpl::} namespace. In addition, \href{https://github.com/AdaptiveCpp/AdaptiveCpp/blob/develop/doc/stdpar.md}{AdaptiveCPP supports C++ parallel algorithms} on Intel GPUs, enabled via the \texttt{-\/-acpp-stdpar} compiler option.
\item \ref{intelstandardfortran}: Standard language parallelism of Fortran is supported by Intel on their GPUs through the Intel Fortran Compiler \texttt{ifx} (the new, LLVM-based compiler, not the \emph{Classic} version), part of the oneAPI HPC toolkit. In the \href{https://www.intel.com/content/www/us/en/developer/articles/release-notes/fortran-compiler-release-notes.html}{oneAPI update 2022.1}, the \href{https://www.intel.com/content/www/us/en/docs/fortran-compiler/developer-guide-reference/2023-2/do-concurrent.html}{\texttt{do\ concurrent} support} was added and extended in further releases. It can be used via the \texttt{-qopenmp} compiler option together with \texttt{-fopenmp-target-do-concurrent} and \texttt{-fopenmp-targets=spir64}.
\item \ref{intelkokkosc}: No direct support by Intel for Kokkos is available, but \href{https://kokkos.github.io/kokkos-core-wiki/}{Kokkos} supports Intel GPUs through an experimental SYCL backend.
\item \ref{intelalpakac}: Since \href{https://github.com/alpaka-group/alpaka/releases/tag/0.9.0}{v.0.9.0}, \href{https://github.com/alpaka-group/alpaka}{Alpaka} contains experimental SYCL support with which Intel GPUs can be targeted. Also, Alpaka can fall back to an OpenMP backend.
\item \ref{intelpython}: Intel GPUs can be used from Python through three notable packages. First, Intel\textquotesingle s \href{https://github.com/IntelPython/dpctl}{\emph{Data Parallel Control} (dpctl)} implements low-level Python bindings to SYCL functionality. It is available on PyPI as \href{https://pypi.org/project/dpctl/}{\texttt{dpctl}}. Second, a higher level, Intel\textquotesingle s \href{https://github.com/IntelPython/numba-dpex}{\emph{Data-parallel Extension to Numba} (numba-dpex)} supplies an extension to the JIT functionality of Numba to support Intel GPUs. It is available from Anaconda as \href{https://anaconda.org/intel/numba-dpex}{\texttt{numba-dpex}}. Finally, and arguably highest level, Intel\textquotesingle s \href{https://github.com/IntelPython/dpnp}{\emph{Data Parallel Extension for Numpy} (dpnp)} builds up on the Numpy API and extends some functions with Intel GPU support. It is available on PyPI as \href{https://pypi.org/project/dpnp/}{\texttt{dpnp}}, although latest versions appear to be available \href{https://github.com/IntelPython/dpnp/releases}{only on GitHub}.
